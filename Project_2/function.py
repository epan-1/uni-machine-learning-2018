###
# This Python module contains helper functions for pre-processing the text data
# given in the datasets
###

import re
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, \
    precision_score, recall_score
import itertools
import numpy as np
import matplotlib.pyplot as plt


def pre_process(data):
    """
    The function will do the following to the data.
        - Force all characters to be lower case
        - Remove any non-alphanumeric characters from the string.
    :param data: Data is a string representing the blog post
    :return: Another string representing the pre-processed data
    """
    # Uses regex '\W+' to remove any character that is not alphabetical or a
    # number.
    output = ''
    for word in data.split(' '):
        if word != '' and word[0].isalnum():
            output += ' ' + re.sub('[\W_]+', '', word.lower(), flags=re.UNICODE)
    return output


def convert_age(raw):
    """
    This function takes the age class and puts it into the specified range given
    by the
    :param raw: A string containing the self-identified age of the blog author
    :return: Returns the corresponding range that the age is in or ? if it
             does not fit into any of the ranges. Inclusive ranges are as follows:
                - 14-16
                - 24-26
                - 34-36
                - 44-46
    """
    if raw == '?':
        # If it's already a question mark skip it
        return '?'
    else:
        if 14 <= int(raw) <= 16:
            return '14-16'
        elif 24 <= int(raw) <= 26:
            return '24-26'
        elif 34 <= int(raw) <= 36:
            return '34-36'
        elif 44 <= int(raw) <= 46:
            return '44-46'
        else:
            return '?'


def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.get_cmap('Wistia')):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    :param cm: This is the confusion matrix generated by sklearn
    :param classes: This refers to a list of all the possible classes
    :param normalize: If set to true, the counts will be normalised with respect
                      to the total number of samples
    :param title: A string to give the title of the plot
    :param cmap: The colors to give the different parts of the confusion matrix
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')


def produce_metrics(learner, X, y, learner_name):
    """
    This function takes a learner implemented from the sklearn library and
    prints out the corresponding metrics for the given instances and true
    class labels:
        - Accuracy score
        - F1-Score which is a more specific version of F-Score where beta=1
        - Precision
        - Recall
        - Confusion matrix
    :param learner: A learner object from the sklearn library
    :param X: An array-like object containing the instances of a given dataset
              to be used for evaluation of the learner
    :param y: The ground truth labels for the corresponding instances in X
    :param learner_name: A string to identify the name of the learner when p
                         printing out metrics
    :return: None
    """
    predictions = learner.predict(X)
    class_names = ['14-16', '24-26', '34-36', '44-46', '?']
    acc_score = accuracy_score(y, predictions)
    f1_val = f1_score(y, predictions, labels=class_names, average='weighted')
    prec = precision_score(y, predictions, labels=class_names, average='weighted')
    rec = recall_score(y, predictions, labels=class_names, average='weighted')
    conf_matrix = confusion_matrix(y, predictions, labels=class_names)

    print("Accuracy score of {} = {}".format(learner_name, acc_score))
    print("F1 score of {} = {}".format(learner_name, f1_val))
    print("Precision score of {} = {}".format(learner_name, prec))
    print("Recall score of {} = {}".format(learner_name, rec))
    plt.figure()
    plot_confusion_matrix(conf_matrix, class_names,
                          title='Confusion Matrix for {}'.format(learner_name))
    plt.show()

    return





