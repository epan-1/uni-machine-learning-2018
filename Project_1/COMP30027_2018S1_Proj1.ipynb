{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2018 Semester 1\n",
    "-----\n",
    "## Project 1: What is labelled data worth to Naive Bayes?\n",
    "-----\n",
    "###### Student Name(s): Edmond Pan\n",
    "###### Python version: 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the seven functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional libraries used in the Naive Bayes implementation\n",
    "import numpy as np\n",
    "import math as mth\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"\n",
    "        Reads in the csv file into an appropriate data structure\n",
    "        :param filename: Name of the .csv file\n",
    "        \"\"\"\n",
    "        # Variables to store metadata about the table structure\n",
    "        self.num_rows = 0\n",
    "        self.num_cols = 0\n",
    "        self.table = []\n",
    "        file = open('2018S1-proj1_data/' + filename, 'r')\n",
    "        for line in file.readlines():\n",
    "            # Split based on common to get the values\n",
    "            row = line.split(',')\n",
    "            self.num_cols = len(row)\n",
    "            # Add row to table and increment row count\n",
    "            self.table.append(row)\n",
    "            self.num_rows += 1\n",
    "        file.close()\n",
    "\n",
    "    def get_num_rows(self):\n",
    "        \"\"\"\n",
    "        Gets the number of rows in the table\n",
    "        :return: Integer referencing number of rows in the table\n",
    "        \"\"\"\n",
    "        return self.num_rows\n",
    "\n",
    "    def get_num_cols(self):\n",
    "        \"\"\"\n",
    "        Gets the number of cols in the table\n",
    "        :return: Integer referring to number of cols in table\n",
    "        \"\"\"\n",
    "        return self.num_cols\n",
    "\n",
    "    def get_table(self):\n",
    "        \"\"\"\n",
    "        Gets the stored table\n",
    "        :return: Returns a list of rows\n",
    "        \"\"\"\n",
    "        return self.table\n",
    "\n",
    "    def random_initial(self):\n",
    "        \"\"\"\n",
    "        Function that replaces the class labels with random (non-uniform)\n",
    "        class distributions. NOTE ONLY USE WHEN DOING UNSUPERVISED NB\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Default dict containing all possible classes\n",
    "        classes = defaultdict(float)\n",
    "        for row in self.table:\n",
    "            # Keep them all at 0 since they will b replaced with\n",
    "            # random fractional counts that sum to 1\n",
    "            classes[row[-1]] = 0\n",
    "\n",
    "        # Now remove the class labels and add the classes dictionaries while\n",
    "        # initialising the values to random fractional counts\n",
    "        for row in self.table:\n",
    "            # Add the random values to the dictionary\n",
    "            values = np.random.dirichlet(np.ones(len(classes)), size=1)\n",
    "            i = 0\n",
    "            for key, value in classes.items():\n",
    "                classes[key] = values[0][i]\n",
    "                i += 1\n",
    "            # Replace the old class label with the random class distribution.\n",
    "            # Make sure to return a copy of classes instead of passing the reference\n",
    "            row[-1] = classes.copy()\n",
    "        return\n",
    "    \n",
    "def print_confusion(predicted, expected):\n",
    "    \"\"\"\n",
    "    Function to print the confusion matrix from a list of predicted and\n",
    "    expected values\n",
    "    :param predicted: A list of predicted values \n",
    "    :param expected:  A list of expected values\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if len(predicted) != len(expected):\n",
    "        print(\"FATAL ERROR: List lengths do not match\")\n",
    "        return\n",
    "    # Structure to store the matrix data. Can access the individual values\n",
    "    # in this format matrix[predicted_class][expected_class]\n",
    "    matrix = defaultdict(lambda: defaultdict(int))\n",
    "    for i in range(len(predicted)):\n",
    "        matrix[predicted[i]][expected[i]] += 1\n",
    "    return matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    \"\"\"\n",
    "    Function that builds a DataSet object from a .csv file containing data\n",
    "    :param filename: A string containing the name of the .csv file\n",
    "    :return: A DataSet object of the .csv file\n",
    "    \"\"\"\n",
    "    return DataSet(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedModel:\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Constructor for a SupervisedModel object\n",
    "        :param dataset: A DataSet object containing the .csv file to calculate\n",
    "                        probabilities from\n",
    "        \"\"\"\n",
    "        # Variables to store counts to use in calculating prior and posterior probabilities\n",
    "        self.prior_counts, self.posterior_counts, self.missing_counts = self.create_counts(dataset)\n",
    "\n",
    "        self.prior_prob, self.posterior_prob = self.__calc_probabilities__(dataset)\n",
    "\n",
    "    def get_prior_counts(self):\n",
    "        return self.prior_counts\n",
    "\n",
    "    def get_posterior_counts(self):\n",
    "        return self.posterior_counts\n",
    "\n",
    "    def get_prior_prob(self):\n",
    "        return self.prior_prob\n",
    "\n",
    "    def get_posterior_prob(self):\n",
    "        return self.posterior_prob\n",
    "\n",
    "    @classmethod\n",
    "    def create_counts(cls, dataset):\n",
    "        \"\"\"\n",
    "        Function to count up the frequencies of the different classes and attributes\n",
    "        in the dataset\n",
    "        :param dataset: A DataSet object to read and produce counts from\n",
    "        :return: 2 dictionaries in tuple form. i.e. (dict1, dict2) that represent the\n",
    "                 counts to be used for prior and posterior probabilities respectively\n",
    "        \"\"\"\n",
    "        prior_counts = defaultdict(int)\n",
    "        # Triple nested dictionary. Key of first dict contains attribute names, second dict\n",
    "        # contains attribute values as keys, third dict contains class names as keys\n",
    "        posterior_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "        # Structure to store how many missing values exist for each attribute. A key refers\n",
    "        # to the attribute name, second key refers to class name\n",
    "        missing_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        # Add prior counts to default dict\n",
    "        for row in dataset.table:\n",
    "            # Assuming last column is the class\n",
    "            prior_counts[row[-1]] += 1\n",
    "        # Add posterior counts to data structure\n",
    "        for row in dataset.table:\n",
    "            row_index = 0\n",
    "            for attribute in row[:-1]:\n",
    "                # If attribute == ?, then add to missing counts dict\n",
    "                if attribute == '?':\n",
    "                    missing_counts[row_index][row[-1]] += 1\n",
    "                else:\n",
    "                    # Initialise all the dictionaries of each possible attribute value\n",
    "                    # to contain all possible classes. Note missing values will not contribute\n",
    "                    # to the counts and are treated as a separate count that will not be used\n",
    "                    for key, value in prior_counts.items():\n",
    "                        posterior_counts[row_index][attribute][key]\n",
    "                row_index += 1\n",
    "            # Now add the counts\n",
    "            row_index = 0\n",
    "            for attribute in row[:-1]:\n",
    "                # Skip adding missing values\n",
    "                if attribute == '?':\n",
    "                    row_index += 1\n",
    "                else:\n",
    "                    # Add to the counts\n",
    "                    posterior_counts[row_index][attribute][row[-1]] += 1\n",
    "                    row_index += 1\n",
    "\n",
    "        return prior_counts, posterior_counts, missing_counts\n",
    "\n",
    "    def __calc_probabilities__(self, dataset):\n",
    "        \"\"\"\n",
    "        Function that takes the counts and produces prior and posterior probabilities\n",
    "        :param dataset: A DataSet object to create the probabilities from\n",
    "        :return: 2 dictionaries in tuple form. i.e. (dict1, dict2) that represent prior\n",
    "                 and posterior probabilities respectively\n",
    "        \"\"\"\n",
    "        prior_prob = defaultdict(float)\n",
    "        # Format for this dict is: First dict key refers to attribute name, second dict key refers\n",
    "        # attribute value and third dict key refers to class name. E.g. P[0]['20-29']['recurrence-events\\n']\n",
    "        posterior_prob = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "        # Values for laplace smoothing\n",
    "        k = 1\n",
    "\n",
    "        # Calculating the prior probabilities\n",
    "        for key, value in self.prior_counts.items():\n",
    "            prior_prob[key] = value/dataset.get_num_rows()\n",
    "\n",
    "        # Calculating the posterior probabilities\n",
    "        for attr_name, val_dict in self.posterior_counts.items():\n",
    "            # Value to add to denominator when doing Laplace smoothing.\n",
    "            unique_attr_num = len(self.posterior_counts[attr_name].items())\n",
    "            for attr_val, class_dict in self.posterior_counts[attr_name].items():\n",
    "                for class_name, count in self.posterior_counts[attr_name][attr_val].items():\n",
    "                    # Do Laplace smoothing of the counts\n",
    "                    numerator = count + k\n",
    "                    denominator = self.prior_counts[class_name] + unique_attr_num - \\\n",
    "                                  self.missing_counts[attr_name][class_name]\n",
    "                    posterior_prob[attr_name][attr_val][class_name] = numerator/denominator\n",
    "\n",
    "        return prior_prob, posterior_prob\n",
    "\n",
    "    \n",
    "def train_supervised(data):\n",
    "    \"\"\"\n",
    "    Function that builds a supervised NB model\n",
    "    :param data: A DataSet object to build a supervised NB model from\n",
    "    :return: A SupervisedModel object\n",
    "    \"\"\"\n",
    "    return SupervisedModel(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_max(dictionary):\n",
    "    \"\"\"\n",
    "    Function that returns the key that has the highest value in a dictionary\n",
    "    :param dictionary: Dictionary containing the values to check\n",
    "    :return: A string containing the name of key with the highest value\n",
    "    \"\"\"\n",
    "    return max(dictionary, key=lambda key: dictionary[key])\n",
    "\n",
    "def predict_sup_single(data_instance, model):\n",
    "    \"\"\"\n",
    "    This function takes a single instance and returns a classification\n",
    "    :param data_instance: A list of attribute values\n",
    "    :param model: A SupervisedModel object to use for evaluation\n",
    "    :return: A string corresponding to the most likely class this instance belongs to\n",
    "    \"\"\"\n",
    "    nb_scores = defaultdict(float)\n",
    "    for class_name, value in model.get_prior_counts().items():\n",
    "        prior = model.get_prior_prob()\n",
    "        posterior = model.get_posterior_prob()\n",
    "        nb_scores[class_name] = prior[class_name]\n",
    "        attr_index = 0\n",
    "        for attribute in data_instance:\n",
    "            # If test instance has missing value, skip it\n",
    "            if attribute == '?':\n",
    "                attr_index += 1\n",
    "            else:\n",
    "                nb_scores[class_name] *= posterior[attr_index][attribute][class_name]\n",
    "                attr_index += 1\n",
    "    return arg_max(nb_scores)\n",
    "\n",
    "\n",
    "def log_pred_sup_single(data_instance, model):\n",
    "    \"\"\"\n",
    "    This function takes a single instance and returns a classification\n",
    "    :param data_instance: A list of attribute values\n",
    "    :param model: A SupervisedModel object to use for evaluation\n",
    "    :return: A string corresponding to the most likely class this instance belongs to\n",
    "    \"\"\"\n",
    "    nb_scores = defaultdict(float)\n",
    "    for class_name, value in model.get_prior_counts().items():\n",
    "        prior = model.get_prior_prob()\n",
    "        posterior = model.get_posterior_prob()\n",
    "        nb_scores[class_name] = mth.log(prior[class_name])\n",
    "        attr_index = 0\n",
    "        for attribute in data_instance:\n",
    "            if attribute == '?':\n",
    "                attr_index += 1\n",
    "            else:\n",
    "                nb_scores[class_name] += mth.log(posterior[attr_index][attribute][class_name])\n",
    "                attr_index += 1\n",
    "    return arg_max(nb_scores)\n",
    "\n",
    "\n",
    "def predict_supervised(filename):\n",
    "    \"\"\"\n",
    "    Function that predicts the class for a set of instances\n",
    "    :param filename: The filename of the .csv file to create predictions from\n",
    "    :return: A list of predicted classes with indices corresponding to the row number\n",
    "    \"\"\"\n",
    "    predicted = []\n",
    "    # Build the model from the dataset.\n",
    "    data = DataSet(filename)\n",
    "    model = SupervisedModel(data)\n",
    "    for row in data.table:\n",
    "        # Also skip last attribute as that is the class distribution\n",
    "        classified = predict_sup_single(row[:-1], model)\n",
    "        if classified == log_pred_sup_single(row[:-1], model):\n",
    "            predicted.append(classified)\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_supervised(filename):\n",
    "    \"\"\"\n",
    "    Function that prints the accuracy rating for a given set of predictions and\n",
    "    constructs a confusion matrix\n",
    "    :param filename: The name of the .csv files the predictions were created from\n",
    "    :return: A confusion matrix in the format of a 2D dictionary accessible in the\n",
    "             format matrix[predicted_class][expected_class]\n",
    "    \"\"\"\n",
    "    # Get the expected classes from the dataset\n",
    "    dataset = DataSet(filename)\n",
    "    expected = []\n",
    "    for row in dataset.table:\n",
    "        expected.append(row[-1])\n",
    "    predicted = predict_supervised(filename)\n",
    "    # Now for each instance check if it matches the expected\n",
    "    num_correct = 0\n",
    "    total_instances = len(expected)\n",
    "    curr_inst = 0\n",
    "    for pred in predicted:\n",
    "        if pred == expected[curr_inst]:\n",
    "            num_correct += 1\n",
    "        curr_inst += 1\n",
    "    print('Accuracy = ' + str((num_correct/total_instances) * 100))\n",
    "    return print_confusion(predicted, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupervisedModel:\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Constructor for an UnsupervisedModel\n",
    "        :param dataset: A DataSet object containing the dataset to classify\n",
    "        \"\"\"\n",
    "        # Variables to store prior counts and posterior counts\n",
    "        self.prior_counts, self.posterior_counts, self.missing_counts = self.create_counts(dataset)\n",
    "        # Variables to store prior and posterior probabilities\n",
    "        self.prior_prob, self.posterior_prob = self.__calc_probabilities__(dataset)\n",
    "\n",
    "    def iterate(self, dataset, n=3):\n",
    "        \"\"\"\n",
    "        Function that iteratively assigns new class distributions to dataset calculated\n",
    "        from the current model\n",
    "        :param dataset: A DataSet object to iteratively assign new class distributions\n",
    "        :param n: The number of iterations to perform. Defaults to 3\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for i in range(n):\n",
    "            self.recalculate(dataset)\n",
    "        return\n",
    "\n",
    "    @classmethod\n",
    "    def recalculate(cls, dataset):\n",
    "        \"\"\"\n",
    "        Function that will recalculate all of the class distributions in\n",
    "        dataset and assign more reliable distributions\n",
    "        :param dataset: A DataSet object to be altered\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Contains the current model of the dataset\n",
    "        old_model = UnsupervisedModel(dataset)\n",
    "        # Go through every instance of the dataset and reassign the class distributions\n",
    "        for row in dataset.table:\n",
    "            row[-1] = predict_uns_single(row[:-1], old_model)\n",
    "        return\n",
    "\n",
    "    @classmethod\n",
    "    def create_counts(cls, dataset):\n",
    "        \"\"\"\n",
    "        Function to produce the fractional counts of the different classes and attributes\n",
    "        in the dataset\n",
    "        :param dataset: A DataSet object to read and produce counts from\n",
    "        :return: 2 dictionaries in tuple form. i.e. (dict1, dict2) that represent the\n",
    "                 counts to be used for prior and posterior probabilities respectively\n",
    "        \"\"\"\n",
    "        prior_counts = defaultdict(float)\n",
    "        # Triple nested dictionary. Key of first dict contains attribute names, second dict\n",
    "        # contains attribute values as keys, third dict contains class names as keys\n",
    "        posterior_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "        # Structure to store how many missing values exist for each attribute. A key refers\n",
    "        # to the attribute name, second key refers to class name\n",
    "        missing_counts = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "        # Add prior counts to default dict\n",
    "        for row in dataset.table:\n",
    "            # Add the corresponding fractional count to the correct class\n",
    "            for class_name, value in row[-1].items():\n",
    "                prior_counts[class_name] += value\n",
    "\n",
    "        for row in dataset.table:\n",
    "            row_index = 0\n",
    "            for attribute in row[:-1]:\n",
    "                # If attribute == ?, then add to missing counts dict\n",
    "                if attribute == '?':\n",
    "                    for class_name, value in row[-1].items():\n",
    "                        missing_counts[row_index][class_name] += value\n",
    "                else:\n",
    "                    # Initialise all the dictionaries of each possible attribute value\n",
    "                    # to contain all possible classes. Note missing values will not contribute\n",
    "                    # to the counts and are treated as a separate count that will not be used\n",
    "                    for key, value in prior_counts.items():\n",
    "                        posterior_counts[row_index][attribute][key]\n",
    "                row_index += 1\n",
    "            # Now add the counts\n",
    "            row_index = 0\n",
    "            for attribute in row[:-1]:\n",
    "                # Skip adding missing values\n",
    "                if attribute == '?':\n",
    "                    row_index += 1\n",
    "                else:\n",
    "                    # Add to the counts\n",
    "                    for class_name, value in row[-1].items():\n",
    "                        posterior_counts[row_index][attribute][class_name] += value\n",
    "                    row_index += 1\n",
    "\n",
    "        return prior_counts, posterior_counts, missing_counts\n",
    "\n",
    "    def __calc_probabilities__(self, dataset):\n",
    "        \"\"\"\n",
    "        Function that takes the counts and produces prior and posterior probabilities\n",
    "        :param dataset: A DataSet object to create the probabilities from\n",
    "        :return: 2 dictionaries in tuple form. i.e. (dict1, dict2) that represent prior\n",
    "                 and posterior probabilities respectively\n",
    "        \"\"\"\n",
    "        prior_prob = defaultdict(float)\n",
    "        # Format for this dict is: First dict key refers to attribute name, second dict key refers\n",
    "        # attribute value and third dict key refers to class name. E.g. P[0]['20-29']['recurrence-events\\n']\n",
    "        posterior_prob = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "\n",
    "        # Calculating the prior probabilities by dividing by the number of instances\n",
    "        for class_name, value in self.prior_counts.items():\n",
    "            prior_prob[class_name] = value/dataset.get_num_rows()\n",
    "\n",
    "        # Now calculate the posterior probabilities by dividing the fractional counts\n",
    "        # by the fractional class counts\n",
    "        for attr_name, val_dict in self.posterior_counts.items():\n",
    "            for attr_val, class_dict in self.posterior_counts[attr_name].items():\n",
    "                for class_name, count in self.posterior_counts[attr_name][attr_val].items():\n",
    "                    numerator = count\n",
    "                    # Subtract the fractional counts from the total if this attribute of\n",
    "                    # the instance is missing\n",
    "                    denominator = self.prior_counts[class_name] - self.missing_counts[attr_name][class_name]\n",
    "                    posterior_prob[attr_name][attr_val][class_name] = numerator/denominator\n",
    "\n",
    "        return prior_prob, posterior_prob\n",
    "    \n",
    "    \n",
    "def train_unsupervised(data):\n",
    "    \"\"\"\n",
    "    Function that builds an unsupervised NB model\n",
    "    :param data: A DataSet object to build the model the unsupervised NB model from\n",
    "    :return: An UnsupervisedModel object\n",
    "    \"\"\"\n",
    "    return UnsupervisedModel(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_uns_single(data_instance, model):\n",
    "    \"\"\"\n",
    "    This function uses a trained unsupervised model to predict the class distribution\n",
    "    for test instance\n",
    "    :param data_instance: A list of attributes to predict the class distribution for \n",
    "    :param model: An UnsupervisedModel object that will be used to make the predictions\n",
    "    :return: A dictionary containing a new class distribution for the test_instance\n",
    "    \"\"\"\n",
    "    class_dist = defaultdict(float)\n",
    "    for class_name, value in model.prior_counts.items():\n",
    "        prior = model.prior_prob\n",
    "        posterior = model.posterior_prob\n",
    "        class_dist[class_name] = prior[class_name]\n",
    "        attr_index = 0\n",
    "        for attribute in data_instance:\n",
    "            # If the data instance has a missing value, skip it during\n",
    "            # calculations\n",
    "            if attribute == '?':\n",
    "                attr_index += 1\n",
    "            else:\n",
    "                class_dist[class_name] *= posterior[attr_index][attribute][class_name]\n",
    "                attr_index += 1\n",
    "    # Now normalise the values to get the new class distribution.\n",
    "    # Create a copy to allow the original class_dist to be modified\n",
    "    temp_values = class_dist.copy()\n",
    "    for class_name, value in temp_values.items():\n",
    "        class_dist[class_name] = value/sum(temp_values.values())\n",
    "\n",
    "    return class_dist\n",
    "\n",
    "\n",
    "def predict_unsupervised(filename, n=3):\n",
    "    \"\"\"\n",
    "    Function that predicts the class for a set of instances\n",
    "    :param filename: The filename of the .csv file that contains the set of instances\n",
    "                     to make predictions from\n",
    "    :param n: The number of iterations to perform. Defaults to 3\n",
    "    :return: A list of predicted classes with indices corresponding to the row number\n",
    "    \"\"\"\n",
    "    predicted = []\n",
    "    # Read and build the model from the dataset.\n",
    "    data = DataSet(filename)\n",
    "    data.random_initial()\n",
    "    model = UnsupervisedModel(data)\n",
    "    # Iterate over the specified amount of times\n",
    "    model.iterate(data, n)\n",
    "    model = UnsupervisedModel(data)\n",
    "    for row in data.table:\n",
    "        # Also skip last attribute as that is the class distribution\n",
    "        predicted.append(arg_max(predict_uns_single(row[:-1], model)))\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_unsupervised(filename, n=3):\n",
    "    \"\"\"\n",
    "    Function that prints the accuracy rating for a given dataset when using Naive Bayes\n",
    "    to classify it's instances. NOTE: It uses all instances in the dataset to train and\n",
    "    will also be testing on the same instances.\n",
    "    :param filename: filename of the dataset to test on\n",
    "    :param n: number of times to iterate when building the model. Default is 3\n",
    "    :return: A confusion matrix in the format of a 2D dictionary accessible in the\n",
    "             format matrix[predicted_class][expected_class]\n",
    "    \"\"\"\n",
    "    predicted = predict_unsupervised(filename, n)\n",
    "    expected = []\n",
    "    data = DataSet(filename)\n",
    "    for row in data.table:\n",
    "        expected.append(row[-1])\n",
    "    matrix = print_confusion(predicted, expected)\n",
    "    accuracy = 0\n",
    "    total_instances = data.get_num_rows()\n",
    "    for key, value in matrix.items():\n",
    "        accuracy += max(matrix[key].values())\n",
    "    print('Accuracy = ' + str((accuracy/total_instances) * 100))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. Since we’re starting off with random guesses, it might be surprising that the unsupervised NB works at all. Explain what characteristics of the data cause it to work pretty well (say, within 10% Accuracy of the supervised NB) most of the time; also, explain why it utterly fails sometimes.\n",
    "2. When evaluating supervised NB across the four different datasets, you will observe some variation in effectiveness (e.g. Accuracy). Explain what causes this variation. Describe and explain any particularly suprising results.\n",
    "3. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out (hint: check out numpy.shuffle()) or cross–validation evaluation strategy. How does your estimate of Accuracy change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "4. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Do you notice any variation in the predictions made by either the supervised or unsupervised NB classifiers? Explain why, or why not.\n",
    "5. The lecture suggests that deterministically labelling the instances in the initialisation phase of the unsupervised NB classifier “doesn’t work very well”. Confirm this for yourself, and then demonstrate why.\n",
    "6. Rather than evaluating the unsupervised NB classifier by assigning a class deterministically, instead calculate how far away the probabilistic estimate of the true class is from 1 (where we would be certain of the correct class), and take the average over the instances. Does this performance estimate change, as we alter the number of iterations in the method? Explain why.\n",
    "7. Explore what causes the unsupervised NB classifier to converge: what proportion of instances change their prediction from the random assignment, to the first iteration? From the first to the second? What is the latest iteration where you observe a prediction change? Make some conjecture(s) as to what is occurring here.\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question. Groups of 2 students should respond to question (1), and three other questions. Your responses should be about 100-200 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 1 Answer\n",
    "\n",
    "Put answer to question 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 Answer\n",
    "\n",
    "Put answer to question 2 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 95.88872476612507\n"
     ]
    }
   ],
   "source": [
    "sup_ans = evaluate_supervised('mushroom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.0\n"
     ]
    }
   ],
   "source": [
    "uns_ans = evaluate_unsupervised('flu-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
