{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Import text datasets from scikit learn\n",
    "# Q1a). and Q1b).\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Newsgroups to access\n",
    "categories = ['alt.atheism','talk.religion.misc']\n",
    "# Get the train subset of \n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "shuffle=True, random_state=30027)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "shuffle=True, random_state=30027)\n",
    "X_train = data_train.data\n",
    "y_train = data_train.target\n",
    "X_test = data_test.data\n",
    "y_test = data_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1c\n",
    "It is not possible to determine the class from inspecting the text alone especially using human eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2a). & Q2b).\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectoriser = CountVectorizer()\n",
    "X_train_cv = vectoriser.fit_transform(X_train)\n",
    "X_test_cv = vectoriser.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents is: 857\n",
      "Number of unique words is: 18089\n",
      "Documents is: 570\n",
      "Number of unique words is: 18089\n"
     ]
    }
   ],
   "source": [
    "# Q2c).\n",
    "print(\"Documents is: \" + str(np.shape(X_train_cv)[0]) + \"\\n\" + \"Number of unique words is: \" + str(np.shape(X_train_cv)[1]))\n",
    "print(\"Documents is: \" + str(np.shape(X_test_cv)[0]) + \"\\n\" + \"Number of unique words is: \" + str(np.shape(X_test_cv)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2d).\n",
    "No there are no documents in X_test whose values are all 0  \n",
    "Also there is no way a document would have values which are all 0 unless a prespecified vocabulary of words\n",
    "is given to the vectoriser. Since a document with values that are all 0 means that the document contains all \n",
    "new unique words that were not present in the a-priori dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "x2 = SelectKBest(chi2, k=10)\n",
    "x2.fit(X_train_cv, y_train)\n",
    "# these two statements can be combined into\n",
    "X_train_x2 = x2.transform(X_train_cv) # a single statement via fit_transform()\n",
    "X_test_x2 = x2.transform(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For X_train: (857, 10)\n",
      "For X_test: (570, 10)\n"
     ]
    }
   ],
   "source": [
    "# Q3a).\n",
    "print(\"For X_train: \" + str(np.shape(X_train_x2)))\n",
    "print(\"For X_test: \" + str(np.shape(X_test_x2)))\n",
    "\n",
    "# So the shape of the 2 sets remain the same in terms of the number of samples (i.e. documents) but the number of \n",
    "# features that they have (i.e. The unique words) has been reduced to the top 10 according to the scoring of\n",
    "# Chi-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atheism\n",
      "atheist\n",
      "atheists\n",
      "brian\n",
      "caltech\n",
      "christ\n",
      "islamic\n",
      "jesus\n",
      "keith\n",
      "ra\n"
     ]
    }
   ],
   "source": [
    "# Q3b).\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(vectoriser.get_feature_names()[feat_num])\n",
    "    \n",
    "# Looks kinda right since we have selected documents relating to religion. But it does seem like it relates to the bias in Chi-Squared. \n",
    "# In the context of these documents, it seems that the top 10 best features appear to be random numbers that rarely appear throughout \n",
    "# all of the documents sampled but will always frequently appear with the same given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allan\n",
      "atheism\n",
      "atheists\n",
      "caltech\n",
      "cco\n",
      "it\n",
      "keith\n",
      "of\n",
      "schneider\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "# Q3c).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(mutual_info_classif, k=10)\n",
    "mi.fit(X_train_cv,y_train)\n",
    "X_train_mi = mi.transform(X_train_cv)\n",
    "X_test_mi = mi.transform(X_test_cv)\n",
    "\n",
    "# Print out top 10 features calculated by MI\n",
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(vectoriser.get_feature_names()[feat_num])\n",
    "    \n",
    "# This one would seem more intuitive as MI would bias features that appear commonly but may not be informative\n",
    "# as to which class they actually refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-NN Accuracy = 0.6578947368421053\n",
      "DT Accuracy = 0.8087719298245614\n",
      "NB Accuracy = 0.8456140350877193\n"
     ]
    }
   ],
   "source": [
    "# Q4a). Building a classifier of the text using K-NN and possibly NB or Decision trees\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# K neighbours\n",
    "k = 5\n",
    "knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n",
    "knn.fit(X_train_cv, y_train)\n",
    "print(\"{}-NN Accuracy = \".format(k) + str(knn.score(X_test_cv, y_test)))\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=None)\n",
    "dt.fit(X_train_cv, y_train)\n",
    "print(\"DT Accuracy = \" + str(dt.score(X_test_cv, y_test)))\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_cv, y_train)\n",
    "print(\"NB Accuracy = \" + str(nb.score(X_test_cv, y_test)))\n",
    "\n",
    "# Clearly Naive Bayes predicts this data the best which might be due to the inherent fact that frequencies\n",
    "# and probabilities when trying to classify textual information is a good approach. Decision tress have slightly\n",
    "# less accuracy but K-Nearest Neighbours performs very mediocre regardless of the value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-NN Accuracy = 0.5491228070175439\n",
      "DT Accuracy = 0.5877192982456141\n",
      "NB Accuracy = 0.6140350877192983\n"
     ]
    }
   ],
   "source": [
    "# Q4b). Using the top 10 features\n",
    "k = 25\n",
    "knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n",
    "knn.fit(X_train_mi, y_train)\n",
    "print(\"{}-NN Accuracy = \".format(k) + str(knn.score(X_test_mi, y_test)))\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=None)\n",
    "dt.fit(X_train_mi, y_train)\n",
    "print(\"DT Accuracy = \" + str(dt.score(X_test_mi, y_test)))\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_mi, y_train)\n",
    "print(\"NB Accuracy = \" + str(nb.score(X_test_mi, y_test)))\n",
    "\n",
    "# Here the accuracy drops dramatically. With the effectiveness of each of the 3 classifiers becoming more \n",
    "# and more similar to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for Mutual Information with cutoff = 15000\n",
      "25-NN Accuracy = 0.656140350877193\n",
      "DT Accuracy = 0.7982456140350878\n",
      "NB Accuracy = 0.8456140350877193\n",
      "\n",
      "Accuracies for Chi-Squared with cutoff = 15000\n",
      "25-NN Accuracy = 0.6736842105263158\n",
      "DT Accuracy = 0.7894736842105263\n",
      "NB Accuracy = 0.843859649122807\n"
     ]
    }
   ],
   "source": [
    "# Q4c). Adjusting K best cutoff and checking between MI and Chi-squared\n",
    "cutoff = 15000\n",
    "\n",
    "mi = SelectKBest(mutual_info_classif, k=cutoff)\n",
    "mi.fit(X_train_cv,y_train)\n",
    "X_train_mi = mi.transform(X_train_cv)\n",
    "X_test_mi = mi.transform(X_test_cv)\n",
    "\n",
    "print(\"Accuracies for Mutual Information with cutoff = {}\".format(cutoff))\n",
    "\n",
    "k = 25\n",
    "knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n",
    "knn.fit(X_train_mi, y_train)\n",
    "print(\"{}-NN Accuracy = \".format(k) + str(knn.score(X_test_mi, y_test)))\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=None)\n",
    "dt.fit(X_train_mi, y_train)\n",
    "print(\"DT Accuracy = \" + str(dt.score(X_test_mi, y_test)))\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_mi, y_train)\n",
    "print(\"NB Accuracy = \" + str(nb.score(X_test_mi, y_test)))\n",
    "\n",
    "print()\n",
    "\n",
    "# For Chi-squared\n",
    "x2 = SelectKBest(chi2, k=cutoff)\n",
    "x2.fit(X_train_cv,y_train)\n",
    "X_train_x2 = x2.transform(X_train_cv)\n",
    "X_test_x2 = x2.transform(X_test_cv)\n",
    "\n",
    "print(\"Accuracies for Chi-Squared with cutoff = {}\".format(cutoff))\n",
    "\n",
    "k = 25\n",
    "knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n",
    "knn.fit(X_train_x2, y_train)\n",
    "print(\"{}-NN Accuracy = \".format(k) + str(knn.score(X_test_x2, y_test)))\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=None)\n",
    "dt.fit(X_train_x2, y_train)\n",
    "print(\"DT Accuracy = \" + str(dt.score(X_test_x2, y_test)))\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_x2, y_train)\n",
    "print(\"NB Accuracy = \" + str(nb.score(X_test_x2, y_test)))\n",
    "\n",
    "# No, it is not possible to improve upon the accuracies of all the models. The best accuracy obtained from all \n",
    "# models will be those that are trained using all of the available features(attributes). Observations show\n",
    "# that as the cutoff of SelectKBest increases so does the accuracy of all 3 models with some showing larger\n",
    "# improvement gains than others.\n",
    "\n",
    "# Also the choice between Chi-squared and Mutual Information seems to have a negligible influence on the\n",
    "# accuracy of all 3 models. For KNN, chi-squared seems to give a slightly better accuracy whereas for the \n",
    "# other 2 models, the choice of error function does not seem to alter accuracy all that much with only\n",
    "# slight differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
